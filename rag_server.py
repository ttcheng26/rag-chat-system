import json
import re
import time
import os
import shutil
from fastapi import FastAPI, Request, UploadFile, File, HTTPException, BackgroundTasks, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional
from urllib.parse import unquote

import chromadb
from openai import OpenAI
from sentence_transformers import SentenceTransformer
import numpy as np

import query_rag_v3 as my_rag
import main_pipeline_v5 as pipeline
import build_vectordb_v3 as db_builder

from dotenv import load_dotenv
load_dotenv()

processing_status = {}

def process_file_background(file_location: str, filename: str):
    """èƒŒæ™¯è™•ç†æª”æ¡ˆ"""
    try:
        processing_status[filename] = {"status": "processing", "message": "æ­£åœ¨è™•ç†ä¸­..."}
        pipeline.process_single_file(file_location, rag_builder)
        processing_status[filename] = {"status": "completed", "message": "è™•ç†å®Œæˆï¼"}
    except Exception as e:
        processing_status[filename] = {"status": "error", "message": str(e)}

# ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
if not os.path.exists(pipeline.DATA_DIR):
    os.makedirs(pipeline.DATA_DIR)

# --- è¨­å®šå€  ---
DB_PATH = os.getenv("CHROMA_DB_PATH", "./chroma_db")
COLLECTION_NAME = "regulations_rag"
MODEL_PATH = os.getenv("EMBEDDING_MODEL_PATH", "jinaai/jina-embeddings-v3") 

LLM_MODEL = os.getenv("VLLM_MODEL", "ISTA-DASLab/gemma-3-27b-it-GPTQ-4b-128g")
API_BASE = os.getenv("VLLM_API_BASE", "http://localhost:8000/v1")
API_KEY = os.getenv("VLLM_API_KEY", "EMPTY")
MAX_CONTEXT_CHARS = int(os.getenv("MAX_CONTEXT_CHARS", "20000"))
# LLM_MODEL = "RedHatAI/gemma-3-12b-it-FP8-dynamic"


# 1. é€£æ¥ LLM 
print(f"é€£æ¥ vLLM: {LLM_MODEL}")
llm_client = OpenAI(base_url=API_BASE, api_key=API_KEY)

# 2. åˆå§‹åŒ– RAG å»ºåº«å¼•æ“ 
print("åˆå§‹åŒ– RAG å»ºåº«å¼•æ“...")
rag_builder = db_builder.VectorDBBuilder(db_path=DB_PATH, model_path=MODEL_PATH)

# 3. å¾ Builder å–å¾—å…±ç”¨ç‰©ä»¶ 
chroma_client = rag_builder.client
collection = rag_builder.collection
embed_model = rag_builder.ef.model 

print("æ¨¡å‹èˆ‡è³‡æ–™åº«è¼‰å…¥å®Œæˆï¼")

# --- FastAPI App è¨­å®š ---
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None
    temperature: Optional[float] = 0.0 
    max_tokens: Optional[int] = 4096   

# ==========================================
# æª”æ¡ˆç®¡ç† API
# ==========================================
@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "rag-backend"}

@app.get("/files")
def list_files():
    """åˆ—å‡ºç›®å‰çŸ¥è­˜åº«ä¸­çš„æª”æ¡ˆ"""
    try:
        files = []
        if os.path.exists(pipeline.DATA_DIR):
            for f in os.listdir(pipeline.DATA_DIR):
                if not f.startswith('.'):
                    files.append(f)
        return {"files": files}
    except Exception as e:
        return {"error": str(e)}

@app.delete("/files")
def delete_file(filename: str = Query(..., description="è¦åˆªé™¤çš„æª”æ¡ˆåç¨±")):
    """åˆªé™¤æª”æ¡ˆä¸¦å¾å‘é‡è³‡æ–™åº«ç§»é™¤"""
    decoded_filename = unquote(filename)
    print(f"[åˆªé™¤] æº–å‚™åˆªé™¤æª”æ¡ˆ: {decoded_filename}")
    
    try:
        file_path = os.path.join(pipeline.DATA_DIR, decoded_filename)
        
        # 1. åˆªé™¤å¯¦é«”æª”æ¡ˆ
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"[åˆªé™¤] å·²åˆªé™¤æª”æ¡ˆ: {file_path}")
        else:
            print(f"[åˆªé™¤] æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
            raise HTTPException(status_code=404, detail="æª”æ¡ˆä¸å­˜åœ¨")
        
        # 2. å¾å‘é‡è³‡æ–™åº«åˆªé™¤ç›¸é—œè³‡æ–™
        doc_name_without_ext = os.path.splitext(decoded_filename)[0]
        
        try:
            # å˜—è©¦ç”¨å®Œæ•´æª”åæŸ¥è©¢
            results = collection.get(
                where={"source_doc": {"$eq": decoded_filename}},
                include=[]
            )
            
            # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå˜—è©¦ç”¨ä¸å«å‰¯æª”åçš„åç¨±
            if not results['ids']:
                results = collection.get(
                    where={"source_doc": {"$eq": doc_name_without_ext}},
                    include=[]
                )
            
            if results['ids']:
                collection.delete(ids=results['ids'])
                print(f"[åˆªé™¤] å·²å¾å‘é‡è³‡æ–™åº«åˆªé™¤ {len(results['ids'])} ç­†è³‡æ–™")
            else:
                print(f"[åˆªé™¤] å‘é‡è³‡æ–™åº«ä¸­æœªæ‰¾åˆ°ç›¸é—œè³‡æ–™")
                
        except Exception as db_err:
            print(f"[åˆªé™¤] å‘é‡è³‡æ–™åº«åˆªé™¤è­¦å‘Š: {db_err}")
        
        # 3. æ¸…é™¤è™•ç†ç‹€æ…‹è¨˜éŒ„
        if decoded_filename in processing_status:
            del processing_status[decoded_filename]
        
        return {"message": f"æª”æ¡ˆ {decoded_filename} å·²åˆªé™¤", "filename": decoded_filename}
    
    except HTTPException:
        raise
    except Exception as e:
        print(f"[åˆªé™¤] å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload")
async def upload_file(file: UploadFile = File(...), background_tasks: BackgroundTasks = BackgroundTasks()):
    """ä¸Šå‚³æª”æ¡ˆä¸¦è‡ªå‹•è§¸ç™¼ RAG å»ºåº«æµç¨‹"""
    try:
        file_location = os.path.join(pipeline.DATA_DIR, file.filename)
        
        # 1. å„²å­˜æª”æ¡ˆ
        with open(file_location, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        print(f"[ä¸Šå‚³] æª”æ¡ˆå·²å„²å­˜: {file_location}")
        
        # 2. è¨­å®šåˆå§‹ç‹€æ…‹
        processing_status[file.filename] = {"status": "processing", "message": "æª”æ¡ˆå·²æ¥æ”¶ï¼Œé–‹å§‹è™•ç†..."}
        print(f"[ä¸Šå‚³] è¨­å®šç‹€æ…‹: {file.filename} -> processing")
        
        # 3. èƒŒæ™¯åŸ·è¡Œ RAG å»ºåº«
        if background_tasks:
            background_tasks.add_task(process_file_background, file_location, file.filename)
        else:
            # åŒæ­¥åŸ·è¡Œä½œç‚ºå‚™æ¡ˆ
            import threading
            thread = threading.Thread(target=process_file_background, args=(file_location, file.filename))
            thread.start()
        
        return {
            "message": f"æª”æ¡ˆå·²æ¥æ”¶ï¼Œæ­£åœ¨èƒŒæ™¯è™•ç†: {file.filename}", 
            "filename": file.filename, 
            "status": "processing"
        }
    except Exception as e:
        print(f"[ä¸Šå‚³] è™•ç†å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/upload-status")
def get_upload_status(filename: str = Query(..., description="æª”æ¡ˆåç¨±")):
    """æŸ¥è©¢æª”æ¡ˆè™•ç†ç‹€æ…‹"""
    decoded_filename = unquote(filename)
    print(f"[æŸ¥è©¢ç‹€æ…‹] åŸå§‹: {filename}")
    print(f"[æŸ¥è©¢ç‹€æ…‹] è§£ç¢¼å¾Œ: {decoded_filename}")
    print(f"[æŸ¥è©¢ç‹€æ…‹] ç›®å‰ç‹€æ…‹å­—å…¸: {list(processing_status.keys())}")
    
    if decoded_filename in processing_status:
        return processing_status[decoded_filename]
    
    # å¦‚æœæ‰¾ä¸åˆ°ç‹€æ…‹ï¼Œæª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨
    file_path = os.path.join(pipeline.DATA_DIR, decoded_filename)
    if os.path.exists(file_path):
        return {"status": "completed", "message": "æª”æ¡ˆå·²å­˜åœ¨"}
    
    return {"status": "unknown", "message": "æ‰¾ä¸åˆ°æ­¤æª”æ¡ˆçš„è™•ç†è¨˜éŒ„"}


active_sessions = set()


@app.post("/stream-chat")
async def stream_chat(request: ChatRequest):

    # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡çš„ session_id è«‹æ±‚
    if request.session_id and request.session_id in active_sessions:
            print(f"[é–å®š] Session {request.session_id} é‡è¤‡è«‹æ±‚ï¼Œå·²é˜»æ“‹ã€‚")
            raise HTTPException(status_code=429, detail="ä¸Šä¸€ç­†å›ç­”å°šæœªå®Œæˆï¼Œè«‹ç¨å€™ã€‚")

    # lock
    if request.session_id:
        active_sessions.add(request.session_id)
            
    async def event_generator():
        try:
            query = request.message
            print(f"\n[API] æ”¶åˆ°å•é¡Œ: {query}")

            def send_progress(msg):
                            data = {
                                "type": "progress",
                                "content": msg,
                                "session_id": request.session_id,
                                "timestamp": str(time.time())
                            }
                            return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
            

            yield send_progress("æ­£åœ¨åˆ†ææ‚¨çš„å•é¡Œ...")
            # === Step 0: æ™ºæ…§æå–é—œéµå­— ===
            core_keywords = my_rag.get_keywords_via_llm(query)
            expanded_keywords = my_rag.expand_keywords_by_intent(query, core_keywords)
            expanded_keywords = expanded_keywords[:8]
            
            

            # === Step 1: å‘é‡æœå°‹ ===
            
            query_vec = embed_model.encode([query]).tolist()
            vector_results = collection.query(
                query_embeddings=query_vec,
                n_results=150,
                include=['documents', 'metadatas', 'distances']
            )
            
            candidates_map = {}
            if vector_results['documents']:
                for i, doc_id in enumerate(vector_results['ids'][0]):
                    candidates_map[doc_id] = {
                        "doc": vector_results['documents'][0][i],
                        "meta": vector_results['metadatas'][0][i],
                        "distance": vector_results['distances'][0][i],
                        "source": "vector"
                    }

            

            # === Step 2: é—œéµå­—å¼·åˆ¶æœå°‹ ===
            if expanded_keywords:
                for kw in expanded_keywords:
                    try:
                        kw_results = collection.get(
                            where_document={"$contains": kw},
                            limit=50,
                            include=['documents', 'metadatas']
                        )
                        
                        if kw_results['ids']:
                            for i, doc_id in enumerate(kw_results['ids']):
                                if doc_id not in candidates_map:
                                    candidates_map[doc_id] = {
                                        "doc": kw_results['documents'][i],
                                        "meta": kw_results['metadatas'][i],
                                        "distance": None, 
                                        "source": "keyword"
                                    }
                    except Exception as e:
                        pass

            combined_docs = [v["doc"] for v in candidates_map.values()]
            combined_metas = [v["meta"] for v in candidates_map.values()]
            combined_dists = [v["distance"] for v in candidates_map.values()]
            
            # === Step 3: é‡æ’åº (Rerank) ===
            reranked_results = my_rag.advanced_reranker(
                query, combined_docs, combined_metas, combined_dists, 
                top_n=60,
                decay_rate=0.98,
                keywords=core_keywords
            )

            # === Step 4: æ‹¼åœ–é‡çµ„ (Merge) ===
            reranked_results = my_rag.group_and_merge_results(reranked_results)
            
            # === Step 5: Scope Guard (å¹´ä»½éæ¿¾) ===
            year_match = re.search(r"\b(1[0-9]{2})\b", query)
            if year_match and "å ±å‘Š" in query and ("ä¸­" in query or "ä¾æ“š" in query or "æ ¹æ“š" in query):
                y = year_match.group(1)
                reranked_results = [
                    r for r in reranked_results
                    if y in ((r.get("meta", {}) or {}).get("source_doc", ""))
                ]

            # === Step 6: æ§‹å»º Context & æº–å‚™å›å‚³å‰ç«¯æ‰€éœ€çš„ã€Œæœå°‹çµæœã€æ ¼å¼ ===
            context_str = ""
            current_char_count = 0
            knowledge_context = [] # æ”¶é›†çµ¦å‰ç«¯é¡¯ç¤ºç”¨

            print("\n--- åƒè€ƒè³‡æ–™ä¾†æº ---")
            for i, res in enumerate(reranked_results):
                meta = res['meta']
                doc_content = res['doc']
                doc_name = meta.get('source_doc', 'æœªçŸ¥')
                node_type = meta.get('type', meta.get('label', 'æœªçŸ¥'))
                

                title = meta.get('title', doc_name)

                knowledge_context.append({
                    "title": title[:30],
                    "content": doc_content[:100] + "...",
                    "source": doc_name
                })


                # æª¢æŸ¥ Token ä¸Šé™
                if current_char_count + len(doc_content) > MAX_CONTEXT_CHARS:
                    continue 

                if i < 5:    
                    print(f" {i+1}. {doc_name} (åˆ†æ•¸: {res['score']:.3f})")
                
                if node_type == 'MergedTable':
                    context_str += f"{doc_content}\n\n"
                else:
                    context_str += f"ã€ä¾†æºæ–‡ä»¶ï¼š{doc_name}ã€‘\n{doc_content}\n\n"
                    
                current_char_count += len(doc_content)

            search_result_chunk = {
                "type": "search_results",
                "session_id": request.session_id,
                "has_knowledge": len(knowledge_context) > 0,
                "knowledge_count": len(knowledge_context),
                "knowledge_context": knowledge_context,
                "timestamp": str(time.time())
            }
            yield f"data: {json.dumps(search_result_chunk, ensure_ascii=False)}\n\n"


            if not context_str:
                print("æœªé¸å…¥ä»»ä½•è³‡æ–™ã€‚")
                context_str = "æ²’æœ‰æ‰¾åˆ°ç›¸é—œè³‡æ–™ã€‚"
            
            # === Step 7: ç”Ÿæˆå›æ‡‰  ===
            is_speech_request = any(kw in query for kw in ["æ¼”è¬›", "è‡´è©", "è¬›ç¨¿", "è‡´è¾­", "ç™¼è¨€ç¨¿"])

            if is_speech_request:
                print("åµæ¸¬åˆ°æ¼”è¬›ç¨¿éœ€æ±‚...")
                prompt = f"""
    <role>
    ä½ ç¾åœ¨æ˜¯æŸæ”¿åºœæ©Ÿé—œæˆ–å¤§å‹ä¼æ¥­çš„ã€Œå¹•åƒšé•·ã€ï¼Œæ­£åœ¨ç‚ºä½ çš„é¦–é•·æ’°å¯«ä¸€ç¯‡å…¬é–‹å ´åˆçš„è‡´è©ç¨¿ã€‚
    </role>

    <context>
    {context_str}
    </context>

    <style_guide>
    1. **èªæ°£è¨­å®š**ï¼šç©©å¥ã€è‡ªä¿¡ã€å¤§å™¨ã€‚é€™æ˜¯è¦ã€Œå”¸å‡ºä¾†ã€çš„ç¨¿å­ï¼Œè«‹ä½¿ç”¨å£èªé€£æ¥è©ï¼ˆå¦‚ã€Œå„ä½è²´è³“ã€ã€ã€Œæˆ‘å€‘çœ‹åˆ°ã€ã€ã€Œé€™ä»£è¡¨è‘—ã€ï¼‰ï¼Œé¿å…ç”Ÿç¡¬çš„å…¬æ–‡èªå¥ã€‚
    2. **æ•¸æ“šè½‰åŒ–**ï¼šå°‡å†°å†·çš„æ•¸æ“šè½‰åŒ–ç‚ºæ•…äº‹ã€‚ä¾‹å¦‚ä¸è¦èªªã€Œæˆé•·20%ã€ï¼Œè¦èªªã€Œæˆ‘å€‘æˆåŠŸå‰µé€ äº†å…©æˆçš„é¡¯è‘—æˆé•·ã€ã€‚
    3. **æ ¼å¼ç¦å¿Œ**ï¼š**çµ•å°ç¦æ­¢**ä½¿ç”¨ Markdown æ¨™é¡Œç¬¦è™Ÿ (#) æˆ–åˆ—é»ç¬¦è™Ÿ (-/1.)ã€‚æ•´ç¯‡ç¨¿å­å¿…é ˆæ˜¯ç´”æ–‡å­—æ®µè½ã€‚
    4. **ç¯‡å¹…æ§åˆ¶**ï¼šç´„ 800 å­—ï¼Œé©åˆ 3-5 åˆ†é˜çš„æ¼”èªªã€‚
    </style_guide>

    <structure>
    1. **é–‹å ´ (15%)**ï¼šå‘åœ¨å ´è²´è³“ï¼ˆä¾æ“šå•é¡Œæƒ…å¢ƒæ¨æ–·ï¼‰è‡´æ„ï¼Œé»å‡ºä»Šæ—¥ä¸»é¡Œçš„é‡è¦æ€§èˆ‡é¡˜æ™¯ã€‚
    2. **æœ¬æ–‡ (70%)**ï¼š
    - å¼•ç”¨ <context> ä¸­çš„å…·é«”æˆæœï¼ˆå¦‚å» å•†åã€ç”¢å€¼ã€ç²çç´€éŒ„ï¼‰ä½œç‚ºæ”¿ç¸¾/æ¥­ç¸¾è­‰æ˜ã€‚
    - å°‡åˆ†æ•£çš„æ•¸æ“šä¸²è¯æˆä¸€å€‹æ¨å‹•ç”¢æ¥­ç™¼å±•çš„æ•…äº‹ã€‚
    - **æ³¨æ„**ï¼šåªèƒ½å¼•ç”¨è³‡æ–™è£¡æœ‰çš„äº‹å¯¦ï¼Œä¸å¯æé€ ã€‚
    3. **çµèª (15%)**ï¼šé‡ç”³æ ¸å¿ƒåƒ¹å€¼ï¼Œä¸¦æå‡ºå°æœªä¾†çš„æœŸè¨± (Call to Action)ï¼Œä»¥é«˜æ˜‚çš„èªæ°£çµå°¾ã€‚
    </structure>

    <user_instruction>
    æ¼”è¬›ä¸»é¡Œï¼š{query}
    è«‹ä¾æ“šä¸Šè¿°æ¶æ§‹ï¼Œæ’°å¯«ä¸€ä»½å®Œæ•´çš„é€å­—æ¼”è¬›ç¨¿ï¼š
    </user_instruction>
    """

            else:
                # ä¸€èˆ¬ QA æ¨¡å¼
                prompt = f"""
    <role>
    ä½ æ˜¯ä¸€ä½éš¸å±¬æ–¼é«˜å±¤æ±ºç­–å–®ä½çš„ã€Œé¦–å¸­æƒ…å ±åˆ†æå¸«ã€ã€‚ä½ çš„ä»»å‹™æ˜¯åŸºæ–¼æª¢ç´¢åˆ°çš„å…§éƒ¨è³‡æ–™ï¼Œæä¾›ç²¾ç¢ºã€è­‰æ“šå°å‘çš„åˆ†æå ±å‘Šã€‚
    </role>

    <context>
    {context_str}
    </context>

    <rules>
    1. **çµ•å°è­‰æ“šåŸå‰‡**ï¼šæ‰€æœ‰å›ç­”å¿…é ˆåš´æ ¼åŸºæ–¼ <context> å…§å®¹ã€‚è‹¥è³‡æ–™ä¸­æœªæåŠï¼Œè«‹ç›´æ¥å›ç­”ã€Œè³‡æ–™åº«ä¸­ç„¡ç›¸é—œè³‡è¨Šã€ï¼Œç¦æ­¢è‡ªè¡Œè…¦è£œæˆ–ä½¿ç”¨å¤–éƒ¨çŸ¥è­˜ã€‚
    2. **æ•¸æ“šç²¾ç¢ºæ€§**ï¼šå¼•ç”¨æ•¸æ“šæ™‚ï¼ˆé‡‘é¡ã€äººæ•¸ã€ç™¾åˆ†æ¯”ï¼‰ï¼Œå¿…é ˆèˆ‡åŸæ–‡å®Œå…¨ä¸€è‡´ï¼Œä¿ç•™å°æ•¸é»èˆ‡å–®ä½ã€‚
    3. **å¯¦é«”å®Œæ•´æ€§**ï¼šæåˆ°å» å•†ã€æ©Ÿæ§‹æˆ–å°ˆæ¡ˆåç¨±æ™‚ï¼Œè«‹åˆ—å‡ºå…¨åï¼Œä¸¦åœ¨é¦–æ¬¡å‡ºç¾æ™‚ä½¿ç”¨ **ç²—é«”** æ¨™ç¤ºã€‚
    4. **æ’é™¤ç„¡æ•ˆè³‡è¨Š**ï¼šè‹¥è¡¨æ ¼æ•¸æ“šç‚º "-" æˆ– "NA"ï¼Œä»£è¡¨ç„¡æ•¸æ“šï¼Œè«‹å‹¿å°‡å…¶è¦–ç‚º "0" æˆ–ç´å…¥çµ±è¨ˆã€‚
    5. **æ™‚é–“æ•æ„Ÿåº¦**ï¼šè‹¥è³‡æ–™åŒ…å«å¤šå€‹å¹´ä»½ï¼ˆå¦‚ 112å¹´ã€113å¹´ï¼‰ï¼Œè«‹å‹™å¿…åœ¨å›ç­”ä¸­æ¨™è¨»å¹´ä»½ã€‚
    6. **æ•¸æ“šé‚è¼¯æª¢æ ¸**ï¼šè‹¥åŸæ–‡è¡¨æ ¼ä¸­åŒ…å«ã€Œç¸½è¨ˆã€æˆ–ã€Œåˆè¨ˆã€ï¼Œè«‹å„ªå…ˆå¼•ç”¨è©²æ•¸å€¼ã€‚è‹¥ä½ éœ€è¦åŠ ç¸½å¤šå€‹å¹´ä»½çš„æ•¸å­—ï¼Œè«‹å‹™å¿…å…ˆé€²è¡Œæ•¸å­¸é©—ç®—ï¼›è‹¥é©—ç®—çµæœèˆ‡åŸæ–‡ç¸½æ•¸ä¸ç¬¦ï¼Œè«‹å›ç­”ã€ŒåŸæ–‡æ•¸æ“šèˆ‡ç¸½è¨ˆæœ‰å‡ºå…¥ã€ï¼Œç¦æ­¢è‡ªè¡Œä¿®æ­£æˆ–è…¦è£œã€‚
    </rules>

    <output_format>
    è«‹ç›´æ¥å›ç­”ä½¿ç”¨è€…çš„å•é¡Œï¼Œç„¡éœ€é–‹å ´ç™½ï¼ˆå¦‚"ä½ å¥½"ã€"æ ¹æ“šè³‡æ–™"ï¼‰ï¼Œä¸¦ä¾ç…§ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼š

    ### ğŸ¯ æ ¸å¿ƒçµè«–
    (ç”¨ 2-3 å¥è©±ç›´æ¥å›ç­”å•é¡Œçš„é‡é»çµè«–)

    ### ğŸ“Š è©³ç´°åˆ†æ
    * **[åˆ†é¡æ¨™é¡Œ 1]**ï¼šå…·é«”èªªæ˜ï¼ŒåŒ…å«å» å•†åå–®èˆ‡é—œéµæ•¸æ“šã€‚
    * **[åˆ†é¡æ¨™é¡Œ 2]**ï¼šå…·é«”èªªæ˜ï¼ŒåŒ…å«å» å•†åå–®èˆ‡é—œéµæ•¸æ“šã€‚
    (è«‹æ ¹æ“šå…§å®¹è‡ªå‹•åˆ†é¡ï¼Œæ¯å€‹é‡é»ä¸€æ®µï¼Œæ¢ç†åˆ†æ˜)

    ### ğŸ’¡ ç¶œåˆè©•ä¼°
    (ç¸½çµè©²è­°é¡Œçš„åƒ¹å€¼ã€è¶¨å‹¢æˆ–ç¼ºå£)
    </output_format>

    <user_query>
    {query}
    </user_query>
    """
            full_response_log = ""

            try:
                # ä½¿ç”¨ä¸²æµ (Stream) å›å‚³çµ¦å‰ç«¯
                stream = llm_client.chat.completions.create(
                    model=LLM_MODEL,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=request.temperature,
                    max_tokens=request.max_tokens,
                    stream=True 
                )

                for chunk in stream:
                    content = chunk.choices[0].delta.content
                    if content:
                        full_response_log += content
                        # åŒ…è£æˆå‰ç«¯è¦çš„æ ¼å¼ (Yellow highlight)
                        resp_chunk = {
                            "type": "chunk",
                            "content": content,
                            "session_id": request.session_id,
                            "timestamp": str(time.time())
                        }
                        yield f"data: {json.dumps(resp_chunk, ensure_ascii=False)}\n\n"

                print("\n" + "="*20 + " å®Œæ•´å›ç­”ç´€éŒ„ " + "="*20)
                print(full_response_log)
                print("="*50 + "\n")
                yield f"data: [DONE]\n\n"

            except Exception as e:
                print(f"LLM ç”Ÿæˆå¤±æ•—: {e}")
                err_chunk = {
                    "type": "error", 
                    "error": str(e), 
                    "timestamp": str(time.time())
                }
                yield f"data: {json.dumps(err_chunk, ensure_ascii=False)}\n\n"
        except Exception as general_error:
                # æ•æ‰ RAG æµç¨‹(æœå°‹/é‡æ’åº)æœ¬èº«çš„éŒ¯èª¤
                print(f"RAG æµç¨‹éŒ¯èª¤: {general_error}")
                err_chunk = {"type": "error", "error": f"ç³»çµ±å…§éƒ¨éŒ¯èª¤: {str(general_error)}", "timestamp": str(time.time())}
                yield f"data: {json.dumps(err_chunk, ensure_ascii=False)}\n\n"
        finally:
                if request.session_id and request.session_id in active_sessions:
                    active_sessions.remove(request.session_id)
                    print(f"[è§£é–] Session {request.session_id} è™•ç†çµæŸã€‚")        

    return StreamingResponse(event_generator(), media_type="text/event-stream")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
